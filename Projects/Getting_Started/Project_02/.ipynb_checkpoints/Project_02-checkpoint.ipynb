{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This project is an exploration of the \"Travel Review Ratings\" data set, obtained from the UCI Machine Learning Repository.\n",
    "# It consists of the following two elements:\n",
    "#     Visualizations of the data set + some statistics computations\n",
    "#     Application of basic machine learning algorithms and machine learning concepts\n",
    "\n",
    "# Some image files in report are not present in this notebook because of the long runtime of some of the sections \n",
    "#     involved in processing the data the images are created from, and the fact that I have to rerun the entire notebook\n",
    "#     every time I close/reopen it (so I comment out these computationally-expensive sections, hence images not created).\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import KNearestNeighbors\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Number of bad values = 1'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Indices of bad values = [2712]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique ID</th>\n",
       "      <th>Churches</th>\n",
       "      <th>Resorts</th>\n",
       "      <th>Beaches</th>\n",
       "      <th>Parks</th>\n",
       "      <th>Theatres</th>\n",
       "      <th>Museums</th>\n",
       "      <th>Malls</th>\n",
       "      <th>Zoo</th>\n",
       "      <th>Restaurants</th>\n",
       "      <th>...</th>\n",
       "      <th>Art galleries</th>\n",
       "      <th>Dance clubs</th>\n",
       "      <th>Swimming pools</th>\n",
       "      <th>Gyms</th>\n",
       "      <th>Bakeries</th>\n",
       "      <th>Beauty &amp; spas</th>\n",
       "      <th>Cafes</th>\n",
       "      <th>View points</th>\n",
       "      <th>Monuments</th>\n",
       "      <th>Gardens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>User 1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.63</td>\n",
       "      <td>3.65</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.92</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.35</td>\n",
       "      <td>2.33</td>\n",
       "      <td>...</td>\n",
       "      <td>1.74</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>User 2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.63</td>\n",
       "      <td>3.65</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.92</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.64</td>\n",
       "      <td>2.33</td>\n",
       "      <td>...</td>\n",
       "      <td>1.74</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>User 3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.63</td>\n",
       "      <td>3.63</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.92</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.64</td>\n",
       "      <td>2.33</td>\n",
       "      <td>...</td>\n",
       "      <td>1.74</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>User 4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.63</td>\n",
       "      <td>3.63</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.92</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.35</td>\n",
       "      <td>2.33</td>\n",
       "      <td>...</td>\n",
       "      <td>1.74</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>User 5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.63</td>\n",
       "      <td>3.63</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.92</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.64</td>\n",
       "      <td>2.33</td>\n",
       "      <td>...</td>\n",
       "      <td>1.74</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unique ID  Churches  Resorts  Beaches  Parks  Theatres  Museums  Malls  \\\n",
       "0    User 1       0.0      0.0     3.63   3.65       5.0     2.92    5.0   \n",
       "1    User 2       0.0      0.0     3.63   3.65       5.0     2.92    5.0   \n",
       "2    User 3       0.0      0.0     3.63   3.63       5.0     2.92    5.0   \n",
       "3    User 4       0.0      0.5     3.63   3.63       5.0     2.92    5.0   \n",
       "4    User 5       0.0      0.0     3.63   3.63       5.0     2.92    5.0   \n",
       "\n",
       "    Zoo  Restaurants  ...  Art galleries  Dance clubs  Swimming pools  Gyms  \\\n",
       "0  2.35         2.33  ...           1.74         0.59             0.5   0.0   \n",
       "1  2.64         2.33  ...           1.74         0.59             0.5   0.0   \n",
       "2  2.64         2.33  ...           1.74         0.59             0.5   0.0   \n",
       "3  2.35         2.33  ...           1.74         0.59             0.5   0.0   \n",
       "4  2.64         2.33  ...           1.74         0.59             0.5   0.0   \n",
       "\n",
       "   Bakeries  Beauty & spas  Cafes  View points  Monuments  Gardens  \n",
       "0       0.5            0.0    0.0          0.0        0.0      0.0  \n",
       "1       0.5            0.0    0.0          0.0        0.0      0.0  \n",
       "2       0.5            0.0    0.0          0.0        0.0      0.0  \n",
       "3       0.5            0.0    0.0          0.0        0.0      0.0  \n",
       "4       0.5            0.0    0.0          0.0        0.0      0.0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Unique ID                 object\n",
       "Churches                 float64\n",
       "Resorts                  float64\n",
       "Beaches                  float64\n",
       "Parks                    float64\n",
       "Theatres                 float64\n",
       "Museums                  float64\n",
       "Malls                    float64\n",
       "Zoo                      float64\n",
       "Restaurants              float64\n",
       "Pubs/bars                float64\n",
       "Local services           float32\n",
       "Burger/pizza shops       float64\n",
       "Hotels/other lodgings    float64\n",
       "Juice bars               float64\n",
       "Art galleries            float64\n",
       "Dance clubs              float64\n",
       "Swimming pools           float64\n",
       "Gyms                     float64\n",
       "Bakeries                 float64\n",
       "Beauty & spas            float64\n",
       "Cafes                    float64\n",
       "View points              float64\n",
       "Monuments                float64\n",
       "Gardens                  float64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2.92"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and clean/format data\n",
    "\n",
    "# Header did not have descriptive names, so got names via RegEx from webpage on dataset.\n",
    "# Also, for some reason, data always loads with an extra column, so I added an extra label to match with it, so\n",
    "#     I can drop that column\n",
    "# Also, the \"Local services\" loads as something other than float (again, unknown reason), so need to recast that column as float\n",
    "attributeNames = ['Unique ID', 'Churches', 'Resorts', 'Beaches', 'Parks', 'Theatres', 'Museums', 'Malls', 'Zoo', 'Restaurants', 'Pubs/bars', 'Local services', 'Burger/pizza shops', 'Hotels/other lodgings', 'Juice bars', 'Art galleries', 'Dance clubs', 'Swimming pools', 'Gyms', 'Bakeries', 'Beauty & spas', 'Cafes', 'View points', 'Monuments', 'Gardens','Extra column']\n",
    "\n",
    "ratingsDataMaster = pd.read_csv('google_review_ratings.csv', sep=',', names=attributeNames, skiprows=[0])\n",
    "\n",
    "# Drop extra column\n",
    "ratingsDataMaster.drop('Extra column', axis=1, inplace=True)\n",
    "\n",
    "# Cast column \"Local services\" to float:\n",
    "col = ratingsDataMaster[\"Local services\"]\n",
    "\n",
    "badValsIndex = []\n",
    "for i in range(col.size):\n",
    "    try:\n",
    "        np.float64(col[i])\n",
    "    except:\n",
    "        badValsIndex.append(i)\n",
    "\n",
    "# See how many values cannot be converted to float\n",
    "display(\"Number of bad values = \" + str(len(badValsIndex)))\n",
    "# See what the indices were\n",
    "display(\"Indices of bad values = \" + str(badValsIndex))\n",
    "ratingsDataMaster.drop(index = badValsIndex, inplace=True)\n",
    "\n",
    "ratingsDataMaster[\"Local services\"] = pd.to_numeric(ratingsDataMaster[\"Local services\"], downcast='float')\n",
    "\n",
    "# Check that data loaded correctly\n",
    "display(ratingsDataMaster.head())\n",
    "display(ratingsDataMaster.dtypes)\n",
    "display(ratingsDataMaster.iloc[3,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Churches</th>\n",
       "      <th>Resorts</th>\n",
       "      <th>Beaches</th>\n",
       "      <th>Parks</th>\n",
       "      <th>Theatres</th>\n",
       "      <th>Museums</th>\n",
       "      <th>Malls</th>\n",
       "      <th>Zoo</th>\n",
       "      <th>Restaurants</th>\n",
       "      <th>Pubs/bars</th>\n",
       "      <th>...</th>\n",
       "      <th>Art galleries</th>\n",
       "      <th>Dance clubs</th>\n",
       "      <th>Swimming pools</th>\n",
       "      <th>Gyms</th>\n",
       "      <th>Bakeries</th>\n",
       "      <th>Beauty &amp; spas</th>\n",
       "      <th>Cafes</th>\n",
       "      <th>View points</th>\n",
       "      <th>Monuments</th>\n",
       "      <th>Gardens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5455.000000</td>\n",
       "      <td>5455.000000</td>\n",
       "      <td>5455.000000</td>\n",
       "      <td>5455.000000</td>\n",
       "      <td>5455.000000</td>\n",
       "      <td>5455.000000</td>\n",
       "      <td>5455.000000</td>\n",
       "      <td>5455.000000</td>\n",
       "      <td>5455.000000</td>\n",
       "      <td>5455.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5455.000000</td>\n",
       "      <td>5455.000000</td>\n",
       "      <td>5455.000000</td>\n",
       "      <td>5455.000000</td>\n",
       "      <td>5455.000000</td>\n",
       "      <td>5455.000000</td>\n",
       "      <td>5455.000000</td>\n",
       "      <td>5455.000000</td>\n",
       "      <td>5455.000000</td>\n",
       "      <td>5454.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.455674</td>\n",
       "      <td>2.319824</td>\n",
       "      <td>2.489520</td>\n",
       "      <td>2.797192</td>\n",
       "      <td>2.959278</td>\n",
       "      <td>2.893809</td>\n",
       "      <td>3.351778</td>\n",
       "      <td>2.541047</td>\n",
       "      <td>3.126301</td>\n",
       "      <td>2.833093</td>\n",
       "      <td>...</td>\n",
       "      <td>2.206060</td>\n",
       "      <td>1.192821</td>\n",
       "      <td>0.949175</td>\n",
       "      <td>0.822374</td>\n",
       "      <td>0.969072</td>\n",
       "      <td>0.999443</td>\n",
       "      <td>0.965098</td>\n",
       "      <td>1.749941</td>\n",
       "      <td>1.530818</td>\n",
       "      <td>1.560570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.827673</td>\n",
       "      <td>1.421542</td>\n",
       "      <td>1.247852</td>\n",
       "      <td>1.309084</td>\n",
       "      <td>1.338948</td>\n",
       "      <td>1.282301</td>\n",
       "      <td>1.413338</td>\n",
       "      <td>1.111338</td>\n",
       "      <td>1.356767</td>\n",
       "      <td>1.307509</td>\n",
       "      <td>...</td>\n",
       "      <td>1.715701</td>\n",
       "      <td>1.107105</td>\n",
       "      <td>0.973623</td>\n",
       "      <td>0.947993</td>\n",
       "      <td>1.202844</td>\n",
       "      <td>1.193097</td>\n",
       "      <td>0.928332</td>\n",
       "      <td>1.598275</td>\n",
       "      <td>1.316172</td>\n",
       "      <td>1.171784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>1.120000</td>\n",
       "      <td>1.110000</td>\n",
       "      <td>1.120000</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.920000</td>\n",
       "      <td>1.360000</td>\n",
       "      <td>1.540000</td>\n",
       "      <td>1.730000</td>\n",
       "      <td>1.770000</td>\n",
       "      <td>1.790000</td>\n",
       "      <td>1.930000</td>\n",
       "      <td>1.620000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>1.640000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.340000</td>\n",
       "      <td>1.910000</td>\n",
       "      <td>2.060000</td>\n",
       "      <td>2.460000</td>\n",
       "      <td>2.670000</td>\n",
       "      <td>2.680000</td>\n",
       "      <td>3.230000</td>\n",
       "      <td>2.170000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>2.680000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.330000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>1.030000</td>\n",
       "      <td>1.070000</td>\n",
       "      <td>1.290000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.810000</td>\n",
       "      <td>2.685000</td>\n",
       "      <td>2.740000</td>\n",
       "      <td>4.095000</td>\n",
       "      <td>4.315000</td>\n",
       "      <td>3.840000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.190000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.530000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.440000</td>\n",
       "      <td>1.160000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.070000</td>\n",
       "      <td>1.560000</td>\n",
       "      <td>1.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Churches      Resorts      Beaches        Parks     Theatres  \\\n",
       "count  5455.000000  5455.000000  5455.000000  5455.000000  5455.000000   \n",
       "mean      1.455674     2.319824     2.489520     2.797192     2.959278   \n",
       "std       0.827673     1.421542     1.247852     1.309084     1.338948   \n",
       "min       0.000000     0.000000     0.000000     0.830000     1.120000   \n",
       "25%       0.920000     1.360000     1.540000     1.730000     1.770000   \n",
       "50%       1.340000     1.910000     2.060000     2.460000     2.670000   \n",
       "75%       1.810000     2.685000     2.740000     4.095000     4.315000   \n",
       "max       5.000000     5.000000     5.000000     5.000000     5.000000   \n",
       "\n",
       "           Museums        Malls          Zoo  Restaurants    Pubs/bars  ...  \\\n",
       "count  5455.000000  5455.000000  5455.000000  5455.000000  5455.000000  ...   \n",
       "mean      2.893809     3.351778     2.541047     3.126301     2.833093  ...   \n",
       "std       1.282301     1.413338     1.111338     1.356767     1.307509  ...   \n",
       "min       1.110000     1.120000     0.860000     0.840000     0.810000  ...   \n",
       "25%       1.790000     1.930000     1.620000     1.800000     1.640000  ...   \n",
       "50%       2.680000     3.230000     2.170000     2.800000     2.680000  ...   \n",
       "75%       3.840000     5.000000     3.190000     5.000000     3.530000  ...   \n",
       "max       5.000000     5.000000     5.000000     5.000000     5.000000  ...   \n",
       "\n",
       "       Art galleries  Dance clubs  Swimming pools         Gyms     Bakeries  \\\n",
       "count    5455.000000  5455.000000     5455.000000  5455.000000  5455.000000   \n",
       "mean        2.206060     1.192821        0.949175     0.822374     0.969072   \n",
       "std         1.715701     1.107105        0.973623     0.947993     1.202844   \n",
       "min         0.000000     0.000000        0.000000     0.000000     0.000000   \n",
       "25%         0.860000     0.690000        0.580000     0.530000     0.520000   \n",
       "50%         1.330000     0.800000        0.740000     0.690000     0.690000   \n",
       "75%         4.440000     1.160000        0.910000     0.840000     0.860000   \n",
       "max         5.000000     5.000000        5.000000     5.000000     5.000000   \n",
       "\n",
       "       Beauty & spas        Cafes  View points    Monuments      Gardens  \n",
       "count    5455.000000  5455.000000  5455.000000  5455.000000  5454.000000  \n",
       "mean        0.999443     0.965098     1.749941     1.530818     1.560570  \n",
       "std         1.193097     0.928332     1.598275     1.316172     1.171784  \n",
       "min         0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "25%         0.540000     0.570000     0.740000     0.790000     0.880000  \n",
       "50%         0.690000     0.760000     1.030000     1.070000     1.290000  \n",
       "75%         0.860000     1.000000     2.070000     1.560000     1.660000  \n",
       "max         5.000000     5.000000     5.000000     5.000000     5.000000  \n",
       "\n",
       "[8 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    1.7\n",
       "1    1.7\n",
       "2    1.7\n",
       "Name: Local services, dtype: float32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display some summary statistics\n",
    "display(ratingsDataMaster.describe())\n",
    "\n",
    "# Verify that the \"Local services\" column is ok\n",
    "display(ratingsDataMaster[\"Local services\"].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Some more basic information\n",
    "\n",
    "# Whether each user is a voter in each category\n",
    "# I am assuming that an average rating of zero means that a user did not vote in that category\n",
    "\n",
    "categoryNames = ratingsDataMaster.columns[1:]\n",
    "numVotersInCategory = ratingsDataMaster.astype(bool).sum(axis=0)\n",
    "barPositions = np.arange(len(categoryNames))\n",
    "\n",
    "plt.barh(barPositions, numVotersInCategory[1:], align='center', alpha=0.5)\n",
    "plt.yticks(barPositions, categoryNames)\n",
    "plt.ylabel(r'Category name')\n",
    "plt.xlabel(r'#Voters in category')\n",
    "plt.title(r'#Voting by category')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.626748722069311,\n",
       " 0.626748722069311,\n",
       " 0.5639566837903581,\n",
       " 0.5639566837903581,\n",
       " 0.5512591866181638,\n",
       " 0.5512591866181638,\n",
       " 0.5367521025063766,\n",
       " 0.5367521025063766,\n",
       " 0.5137902114031897,\n",
       " 0.5137902114031897]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Look at correlation between attributes\n",
    "\n",
    "c = ratingsDataMaster.corr()\n",
    "sns.heatmap(c, xticklabels=False, yticklabels=False)\n",
    "corrList = []\n",
    "for col in c.columns:\n",
    "    for row in c.index:\n",
    "        if col != row:\n",
    "            corrList.append(c.loc[row,col])\n",
    "corrList.sort(key = abs, reverse = True)\n",
    "corrList[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(corrList, 50, density=True, facecolor='g', alpha=0.75)\n",
    "\n",
    "\n",
    "plt.xlabel('Correlation')\n",
    "plt.ylabel('Number')\n",
    "plt.title('Histogram of Correlations Between Variables')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Parks', 'Theatres', 'Museums', 'Dance clubs']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Cafes', 'View points', 'Monuments', 'Gardens']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a function to select attributes that have a low correlation with specified attribute\n",
    "\n",
    "def lowCorr(data, attributeName, cutoff):\n",
    "    \"\"\"\n",
    "    Returns a list of names of attributes that have an absolute correlation of less than \"cutoff\" with specified attributeName\n",
    "    \"\"\"\n",
    "    correlations = data.corr()\n",
    "    lowCorrAttributes = []\n",
    "    for attr in correlations.loc[attributeName].index:\n",
    "        if abs(correlations[attributeName].at[attr]) <= cutoff and attr != attributeName:\n",
    "            lowCorrAttributes.append(attr)\n",
    "    return lowCorrAttributes\n",
    "\n",
    "def highCorr(data, attributeName, cutoff):\n",
    "    \"\"\"\n",
    "    Returns a list of names of attributes that have an absolute correlation of less than \"cutoff\" with specified attributeName\n",
    "    \"\"\"\n",
    "    correlations = data.corr()\n",
    "    highCorrAttributes = []\n",
    "    for attr in correlations.loc[attributeName].index:\n",
    "        if abs(correlations[attributeName].at[attr]) >= cutoff and attr != attributeName:\n",
    "            highCorrAttributes.append(attr)\n",
    "    return highCorrAttributes\n",
    "\n",
    "# Test lowCorr()\n",
    "# Expecting a list containing exactly the elements in: [\"Parks\", \"Theatres\", \"Museums\", \"Dance clubs\"]\n",
    "display(lowCorr(ratingsDataMaster, \"Churches\", 0.1))\n",
    "display(highCorr(ratingsDataMaster, \"Churches\", 0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7398909111749998"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Try using K-Nearest-Neighbors to predict value of an attribute based on other individually uncorrelated attributes\n",
    "# LOW-correlation case\n",
    "\n",
    "currentAttribute = \"Churches\"\n",
    "\n",
    "lowCorrAttributes = lowCorr(ratingsDataMaster, currentAttribute, 0.1)\n",
    "\n",
    "currentData = ratingsDataMaster[lowCorrAttributes].values\n",
    "currentLabels = ratingsDataMaster[currentAttribute].values\n",
    "\n",
    "currentData = [[example,label] for example, label in zip(currentData,currentLabels)]\n",
    "np.random.shuffle(currentData)\n",
    "currentTrainData = currentData[:1000]\n",
    "currentTestData = currentData[1000:1200]\n",
    "\n",
    "KNN1 = KNearestNeighbors.KNearestNeighbors()\n",
    "KNN1.setData(currentTrainData, len(lowCorrAttributes), 200)\n",
    "\n",
    "def avgSquareDiffLoss(predictor, data):\n",
    "    totalLoss = 0\n",
    "    for e in data:\n",
    "        totalLoss += (e[1] - predictor(e[0]))**2\n",
    "    return totalLoss/float(len(data))\n",
    "\n",
    "# Quick verification of avgSquareDiffLoss\n",
    "#display(avgSquareDiffLoss(lambda x: x[0]-x[1], [[[1,2], -1], [[6,1], 5], [[34,3], 31]]))\n",
    "#display(avgSquareDiffLoss(lambda x: x[0]-x[1], [[[1,2], -1], [[6,1], 5], [[34,3], 30]]))\n",
    "#display(avgSquareDiffLoss(lambda x: x[0]-x[1], [[[1,2], 1], [[6,1], 6], [[34,3], 32]]))\n",
    "#display(avgSquareDiffLoss(lambda x: 0, [[[1,2], 1], [[6,1], 6], [[34,3], 32],[[4,3], 0],[[4,3], 0],[[4,3], 0],[[4,3], 0],[[4,3], 0],[[4,3], 0],[[1,3], 0]]))\n",
    "\n",
    "display(avgSquareDiffLoss(KNN1.predict_averageLabels, currentTestData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55159724535"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Try using K-Nearest-Neighbors to predict value of an attribute based on other individually correlated attributes\n",
    "# HIGH-correlation case\n",
    "\n",
    "ratingsDataCopy = ratingsDataMaster.copy()\n",
    "\n",
    "currentAttribute = \"Churches\"\n",
    "\n",
    "highCorrAttributes = highCorr(ratingsDataMaster, currentAttribute, 0.2)\n",
    "\n",
    "currentData = ratingsDataMaster[highCorrAttributes].values\n",
    "currentLabels = ratingsDataMaster[currentAttribute].values\n",
    "\n",
    "currentData = [[example,label] for example, label in zip(currentData,currentLabels)]\n",
    "np.random.shuffle(currentData)\n",
    "currentTrainData = currentData[:1000]\n",
    "currentTestData = currentData[1000:1200]\n",
    "\n",
    "KNN1 = KNearestNeighbors.KNearestNeighbors()\n",
    "KNN1.setData(currentTrainData, len(highCorrAttributes), 200)\n",
    "\n",
    "def avgSquareDiffLoss(predictor, data):\n",
    "    totalLoss = 0\n",
    "    for e in data:\n",
    "        totalLoss += (e[1] - predictor(e[0]))**2\n",
    "    return totalLoss/float(len(data))\n",
    "\n",
    "# Quick verification of avgSquareDiffLoss\n",
    "#display(avgSquareDiffLoss(lambda x: x[0]-x[1], [[[1,2], -1], [[6,1], 5], [[34,3], 31]]))\n",
    "#display(avgSquareDiffLoss(lambda x: x[0]-x[1], [[[1,2], -1], [[6,1], 5], [[34,3], 30]]))\n",
    "#display(avgSquareDiffLoss(lambda x: x[0]-x[1], [[[1,2], 1], [[6,1], 6], [[34,3], 32]]))\n",
    "#display(avgSquareDiffLoss(lambda x: 0, [[[1,2], 1], [[6,1], 6], [[34,3], 32],[[4,3], 0],[[4,3], 0],[[4,3], 0],[[4,3], 0],[[4,3], 0],[[4,3], 0],[[1,3], 0]]))\n",
    "\n",
    "display(avgSquareDiffLoss(KNN1.predict_averageLabels, currentTestData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package previous computation into a function and run on all attributes\n",
    "# Image ref: 01\n",
    "\n",
    "# \"cutoff\" is the correlation threshold for selecting attributes\n",
    "def lossOfKNNOnAttr(data, attribute, k, cutoff):\n",
    "    lowCorrAttributes = lowCorr(data, attribute, cutoff)\n",
    "    currentData = data[lowCorrAttributes].values\n",
    "    currentLabels = data[attribute].values\n",
    "\n",
    "    currentData = [[example,label] for example, label in zip(currentData,currentLabels)]\n",
    "    #np.random.shuffle(currentData)\n",
    "    currentTrainData = currentData[:int(0.8 * len(data.index))]\n",
    "    currentTestData = currentData[int(0.8 * len(data.index)):]\n",
    "\n",
    "    KNN = KNearestNeighbors.KNearestNeighbors()\n",
    "    KNN.setData(currentTrainData, len(lowCorrAttributes), k)\n",
    "\n",
    "    return avgSquareDiffLoss(KNN.predict_averageLabels, currentTestData), len(lowCorrAttributes)\n",
    "\n",
    "def lossOfKNNOnAttrB(data, attribute, k, cutoff):\n",
    "    highCorrAttributes = highCorr(data, attribute, cutoff)\n",
    "    currentData = data[highCorrAttributes].values\n",
    "    currentLabels = data[attribute].values\n",
    "\n",
    "    currentData = [[example,label] for example, label in zip(currentData,currentLabels)]\n",
    "    #np.random.shuffle(currentData)\n",
    "    currentTrainData = currentData[:int(0.8 * len(data.index))]\n",
    "    currentTestData = currentData[int(0.8 * len(data.index)):]\n",
    "\n",
    "    KNN = KNearestNeighbors.KNearestNeighbors()\n",
    "    KNN.setData(currentTrainData, len(highCorrAttributes), k)\n",
    "\n",
    "    return avgSquareDiffLoss(KNN.predict_averageLabels, currentTestData), len(highCorrAttributes)\n",
    "\n",
    "\n",
    "# The following 4+ functions lossOnAllAttributesX() consist of various ways of computing losses of the\n",
    "# KNN algorithm across each attribute, subject to many different combinations of parameters\n",
    "\n",
    "def lossOnAllAttributesA(data, kValues, cutoff):\n",
    "    losses = pd.DataFrame(index=kValues,columns=data.columns)\n",
    "    numLowCorr = pd.Series(index=data.columns) # This will hold the number of low-correlation attributes for each attribute\n",
    "    for attr in losses.columns:\n",
    "        for k in kValues:\n",
    "            losses.at[k,attr], n = lossOfKNNOnAttr(data, attr, k, cutoff)\n",
    "        numLowCorr.at[attr] = n\n",
    "    return losses, numLowCorr\n",
    "\n",
    "def lossOnAllAttributesB(data, kValues, cutoffs):\n",
    "    losses = []\n",
    "    numLowCorr = pd.DataFrame # This will hold the number of low-correlation attributes for each attribute based on the correlation cutoff\n",
    "    for attr in data.columns:\n",
    "        for k in kValues:\n",
    "            for c in cutoffs:\n",
    "                loss, n = lossOfKNNOnAttr(data, attr, k, c)\n",
    "                losses.append([attr,k,c,loss])\n",
    "                numLowCorr.at[attr, c] = n\n",
    "    return pd.DataFrame(losses,columns=['Attribute', 'K', 'Cutoff', 'Loss']), numLowCorr\n",
    "def lossOnAllAttributesC(dataSamples, attributesToUse, kValues, cutoffs):\n",
    "    losses = []\n",
    "    avgNumLowCorr = pd.DataFrame() # This will hold the average number of low-correlation attributes for each attribute across all random samples based on the correlation cutoff\n",
    "    for attr in attributesToUse:\n",
    "        avgNumLowCorr.at[attr] = 0\n",
    "        for k in kValues:\n",
    "            for c in cutoffs:\n",
    "                loss = 0\n",
    "                nAvg = 0\n",
    "                for sample in dataSamples:\n",
    "                    l, n = lossOfKNNOnAttr(sample, attr, k, c)\n",
    "                    loss += l\n",
    "                    nAvg += n\n",
    "                avgNumLowCorr.at[attr, c] = nAvg / float(len(dataSamples))\n",
    "                loss = loss / float(len(dataSamples))\n",
    "                losses.append([attr,k,c,loss])\n",
    "        \n",
    "    return pd.DataFrame(losses,columns=['Attribute', 'K', 'Cutoff', 'Loss']), avgNumLowCorr\n",
    "\n",
    "# Same as previous version, except it tags each with the size of samples, and returns a list instead of a dataframe\n",
    "def lossOnAllAttributesD(dataSamples, attributesToUse, kValues, cutoffs, sampleSize):\n",
    "    losses = []\n",
    "    avgNumLowCorr = pd.DataFrame() # This will hold the average number of low-correlation attributes for each attribute across all random samples based on the correlation cutoff\n",
    "    for attr in attributesToUse:\n",
    "        avgNumLowCorr.at[attr] = 0\n",
    "        for k in kValues:\n",
    "            for c in cutoffs:\n",
    "                loss = 0\n",
    "                nAvg = 0\n",
    "                for sample in dataSamples:\n",
    "                    l, n = lossOfKNNOnAttr(sample, attr, k, c)\n",
    "                    loss += l\n",
    "                    nAvg += n\n",
    "                avgNumLowCorr.at[attr, c] = nAvg / float(len(dataSamples))\n",
    "                loss = loss / float(len(dataSamples))\n",
    "                losses.append([attr,k,c,loss,sampleSize])\n",
    "        \n",
    "    return losses\n",
    "\n",
    "def lossOnAllAttributesE(dataSamples, attributesToUse, kValues, cutoffs):\n",
    "    losses = []\n",
    "    avgNumHighCorr = pd.DataFrame() \n",
    "    for attr in attributesToUse:\n",
    "        avgNumHighCorr.at[attr] = 0\n",
    "        for k in kValues:\n",
    "            for c in cutoffs:\n",
    "                loss = 0\n",
    "                nAvg = 0\n",
    "                for sample in dataSamples:\n",
    "                    l, n = lossOfKNNOnAttrB(sample, attr, k, c)\n",
    "                    loss += l\n",
    "                    nAvg += n\n",
    "                avgNumHighCorr.at[attr, c] = nAvg / float(len(dataSamples))\n",
    "                loss = loss / float(len(dataSamples))\n",
    "                losses.append([attr,k,c,loss])\n",
    "        \n",
    "    return pd.DataFrame(losses,columns=['Attribute', 'K', 'Cutoff', 'Loss']), avgNumHighCorr\n",
    "\n",
    "dataNoIDs = ratingsDataMaster.drop(labels=\"Unique ID\", axis=1)\n",
    "#for attr in dataNoIDs:\n",
    "#    display(attr)\n",
    "#    display(lossOfKNNOnAttr(dataNoIDs.sample(500), attr, 50, 0.1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Can take a bit to run\\n\\nK = 7\\nsamples = 30\\nSS = 200\\n\\nlossDefault = lossOnAllAttributesNoOffset(dataNoIDs, K, samples, sampleSize=SS)\\nlossOffsetA = lossOnAllAttributesOffsetA(dataNoIDs, K, samples, sampleSize=SS, offset=5)\\nlossOffsetB = lossOnAllAttributesOffsetB(dataNoIDs, K, samples, sampleSize=SS)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try offsetting data to account for the possibility that a zero average rating in a category corresponds to no ratings in that category\n",
    "\n",
    "# Uses all attributes for prediction (other than attribute being predicted)\n",
    "def lossOfKNNOnAttrC(data, attribute, k):\n",
    "    currentData = data[[attr for attr in data.columns if attr != attribute]].values\n",
    "    currentLabels = data[attribute].values\n",
    "\n",
    "    currentData = [[example,label] for example, label in zip(currentData,currentLabels)]\n",
    "    #np.random.shuffle(currentData)\n",
    "    currentTrainData = currentData[:int(0.8 * len(data.index))]\n",
    "    currentTestData = currentData[int(0.8 * len(data.index)):]\n",
    "\n",
    "    KNN = KNearestNeighbors.KNearestNeighbors()\n",
    "    KNN.setData(currentTrainData, len(data.columns)-1, k)\n",
    "\n",
    "    return avgSquareDiffLoss(KNN.predict_averageLabels, currentTestData)\n",
    "\n",
    "# Same as above, expect changes the KNN distance function to not account for differences between components if one is zero\n",
    "# Meant to be used with data that has not been offset\n",
    "def lossOfKNNOnAttrOffsetter(data, attribute, k):\n",
    "    currentData = data[[attr for attr in data.columns if attr != attribute]].values\n",
    "    currentLabels = data[attribute].values\n",
    "\n",
    "    currentData = [[example,label] for example, label in zip(currentData,currentLabels)]\n",
    "    #np.random.shuffle(currentData)\n",
    "    currentTrainData = currentData[:int(0.8 * len(data.index))]\n",
    "    currentTestData = currentData[int(0.8 * len(data.index)):]\n",
    "\n",
    "    KNN = KNearestNeighbors.KNearestNeighbors()\n",
    "    KNN.setData(currentTrainData, len(data.columns)-1, k)\n",
    "    KNN.dist = lambda x, y: sum([(x[i]-y[i])**2 for i in range(len(y)) if x[i] != 0 and y[i] != 0])\n",
    "\n",
    "    return avgSquareDiffLoss(KNN.predict_averageLabels, currentTestData)\n",
    "\n",
    "def lossOnAllAttributesNoOffset(data, K, samples, sampleSize=100):\n",
    "    losses = []\n",
    "    for attr in data.columns:\n",
    "        sumLoss = 0\n",
    "        for _ in range(samples):\n",
    "            sumLoss += lossOfKNNOnAttrC(data.sample(sampleSize), attr, k)\n",
    "        avgLoss = sumLoss/float(samples)\n",
    "        losses.append((attr, avgLoss))\n",
    "    return losses\n",
    "\n",
    "def lossOnAllAttributesOffsetA(data, K, samples, sampleSize=100, offset=5):\n",
    "    data = data.copy()\n",
    "    data.apply(lambda x: x.apply(lambda y: y + offset if y > 0 else y))\n",
    "    losses = []\n",
    "    for attr in data.columns:\n",
    "        sumLoss = 0\n",
    "        for _ in range(samples):\n",
    "            sumLoss += lossOfKNNOnAttrC(data.sample(sampleSize), attr, k)\n",
    "        avgLoss = sumLoss/float(samples)\n",
    "        losses.append((attr, avgLoss))\n",
    "    return losses\n",
    "\n",
    "def lossOnAllAttributesOffsetB(data, K, samples, sampleSize=100):\n",
    "    losses = []\n",
    "    for attr in data.columns:\n",
    "        sumLoss = 0\n",
    "        for _ in range(samples):\n",
    "            sumLoss += lossOfKNNOnAttrOffsetter(data.sample(sampleSize), attr, k)\n",
    "        avgLoss = sumLoss/float(samples)\n",
    "        losses.append((attr, avgLoss))\n",
    "    return losses\n",
    "\n",
    "\"\"\"\n",
    "# Can take a bit to run\n",
    "\n",
    "K = 7\n",
    "samples = 30\n",
    "SS = 200\n",
    "\n",
    "lossDefault = lossOnAllAttributesNoOffset(dataNoIDs, K, samples, sampleSize=SS)\n",
    "lossOffsetA = lossOnAllAttributesOffsetA(dataNoIDs, K, samples, sampleSize=SS, offset=5)\n",
    "lossOffsetB = lossOnAllAttributesOffsetB(dataNoIDs, K, samples, sampleSize=SS)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlosses_i = []\\nfor loss_i in [lossDefault, lossOffsetA, lossOffsetB]:\\n    losses_i.append([loss_i[i][1] for i in range(len(loss_i))])\\n\\n#plt.hist(losses_i, label=[\"No Offset\", \"Offset A\", \"Offset B\"], rwidth = 0.6)\\n#plt.legend()\\n\\n\\n#for loss_i, lab in zip(losses_i, [\"No Offset\", \"Offset A\", \"Offset B\"]):\\n#    plt.hist(loss_i, label=lab, edgecolor=(0,0,0,1), linewidth = 2, alpha=0.7)\\n#plt.legend()\\n\\n\\nfor loss_i, lab in zip(losses_i, [\"No Offset\", \"Offset A\", \"Offset B\"]):\\n    plt.plot(list(range(24)), loss_i, label=lab)\\nplt.legend()\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Image ref: \n",
    "\"\"\"\n",
    "losses_i = []\n",
    "for loss_i in [lossDefault, lossOffsetA, lossOffsetB]:\n",
    "    losses_i.append([loss_i[i][1] for i in range(len(loss_i))])\n",
    "\n",
    "#plt.hist(losses_i, label=[\"No Offset\", \"Offset A\", \"Offset B\"], rwidth = 0.6)\n",
    "#plt.legend()\n",
    "\n",
    "\n",
    "#for loss_i, lab in zip(losses_i, [\"No Offset\", \"Offset A\", \"Offset B\"]):\n",
    "#    plt.hist(loss_i, label=lab, edgecolor=(0,0,0,1), linewidth = 2, alpha=0.7)\n",
    "#plt.legend()\n",
    "\n",
    "\n",
    "for loss_i, lab in zip(losses_i, [\"No Offset\", \"Offset A\", \"Offset B\"]):\n",
    "    plt.plot(list(range(24)), loss_i, label=lab)\n",
    "plt.legend()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCan take a bit to run\\n\\nkValues = [1,2,3,4,6,9,15,30,60]\\ncutoffs = [0.06, 0.12, 0.24]\\nnumSamples = 7\\nsampleSize = 120\\n\\n#losses, numLowCorr = lossOnAllAttributesB(dataNoIDs.sample(500), kValues, cutoffs)\\nlosses, numLowCorr = lossOnAllAttributesC([dataNoIDs.sample(sampleSize) for _ in range(numSamples)],\\n                                          dataNoIDs.columns , kValues, cutoffs)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Can take a bit to run\n",
    "\n",
    "kValues = [1,2,3,4,6,9,15,30,60]\n",
    "cutoffs = [0.06, 0.12, 0.24]\n",
    "numSamples = 7\n",
    "sampleSize = 120\n",
    "\n",
    "#losses, numLowCorr = lossOnAllAttributesB(dataNoIDs.sample(500), kValues, cutoffs)\n",
    "losses, numLowCorr = lossOnAllAttributesC([dataNoIDs.sample(sampleSize) for _ in range(numSamples)],\n",
    "                                          dataNoIDs.columns , kValues, cutoffs)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image ref: 01\n",
    "#sns.relplot(x=\"K\", y=\"Loss\", col=\"Attribute\", hue=\"Cutoff\", kind=\"line\", estimator=None, col_wrap=5, height=3, data=losses)\n",
    "#display(numLowCorr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nvariances = []\\naverages = []\\n\\nfor attr in dataNoIDs.columns:\\n    variances.append((attr,losses[losses[\"Attribute\"]==attr].var()[\"Loss\"]))\\n    averages.append((attr,losses[losses[\"Attribute\"]==attr].mean()[\"Loss\"]))\\nvariances.sort(key=lambda x:x[1], reverse=True)\\naverages.sort(key=lambda x:x[1])\\n\\n# Selecting highest 5\\nselectedAttributes = [p[0] for p in variances[:5]]\\nlowLossAttributes = [p[0] for p in averages[:5]]\\n# Result for selectedAttributes was [\\'View points\\', \\'Juice bars\\', \\'Museums\\', \\'Malls\\', \\'Theatres\\']\\n# Result for lowLossAttributes was [\\'Churches\\', \\'Gyms\\', \\'Cafes\\', \\'Bakeries\\', \\'Swimming pools\\']\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting the attributes which vary more over different values of k for closer inspection\n",
    "# Also selecting those which vary more over correlation cutoff\n",
    "# (These characteristics are not selected for independently; the way they are selected, both qualities contribute to selection)\n",
    "# Finally, look at which attributes had lowest loss\n",
    "\"\"\"\n",
    "variances = []\n",
    "averages = []\n",
    "\n",
    "for attr in dataNoIDs.columns:\n",
    "    variances.append((attr,losses[losses[\"Attribute\"]==attr].var()[\"Loss\"]))\n",
    "    averages.append((attr,losses[losses[\"Attribute\"]==attr].mean()[\"Loss\"]))\n",
    "variances.sort(key=lambda x:x[1], reverse=True)\n",
    "averages.sort(key=lambda x:x[1])\n",
    "\n",
    "# Selecting highest 5\n",
    "selectedAttributes = [p[0] for p in variances[:5]]\n",
    "lowLossAttributes = [p[0] for p in averages[:5]]\n",
    "# Result for selectedAttributes was ['View points', 'Juice bars', 'Museums', 'Malls', 'Theatres']\n",
    "# Result for lowLossAttributes was ['Churches', 'Gyms', 'Cafes', 'Bakeries', 'Swimming pools']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCan take a bit to run\\n\\nkValues = [1,2,3,4,4,6,9,15,25,40,60,90]\\ncutoffs = [0.025, 0.05, 0.10, 0.15, 0.20, 0.25, 0.3]\\nnumSamples = 10\\nsampleSize = 110\\n\\n#losses, numLowCorr = lossOnAllAttributesB(dataNoIDs.sample(500), kValues, cutoffs)\\nlosses, numLowCorr = lossOnAllAttributesC([dataNoIDs.sample(sampleSize) for _ in range(numSamples)],\\n                                          selectedAttributes, kValues, cutoffs)\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute more accurate graphs for attributes which had highest variance in loss\n",
    "# Image ref: 02\n",
    "\"\"\"\n",
    "Can take a bit to run\n",
    "\n",
    "kValues = [1,2,3,4,4,6,9,15,25,40,60,90]\n",
    "cutoffs = [0.025, 0.05, 0.10, 0.15, 0.20, 0.25, 0.3]\n",
    "numSamples = 10\n",
    "sampleSize = 110\n",
    "\n",
    "#losses, numLowCorr = lossOnAllAttributesB(dataNoIDs.sample(500), kValues, cutoffs)\n",
    "losses, numLowCorr = lossOnAllAttributesC([dataNoIDs.sample(sampleSize) for _ in range(numSamples)],\n",
    "                                          selectedAttributes, kValues, cutoffs)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image ref: 02\n",
    "# sns.relplot(x=\"K\", y=\"Loss\", col=\"Attribute\", hue=\"Cutoff\", kind=\"line\", col_wrap=2, estimator=None, data=losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCan take a long time to run\\n\\nkValues = [1,2,3,4,4,6,9,15,25,40,60,90]\\ncutoffs = [0.025, 0.05, 0.10, 0.15, 0.20, 0.25, 0.3]\\nnumSamples = 10\\nsampleSizes = [20, 50, 100, 150, 250, 500, 1000]\\n\\nlossesList = []\\nfor sampleSize in sampleSizes:\\n    lossesList.append(lossOnAllAttributesD([dataNoIDs.sample(sampleSize) for _ in range(numSamples)],\\n                                          [\"Restaurants\"], kValues, cutoffs, sampleSize))\\n\\n\\nimport itertools\\nlosses = list(itertools.chain.from_iterable(lossesList))\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Want to see how the results of my method of calculating loss vary for different sample sizes\n",
    "# Image ref: 04\n",
    "\n",
    "\"\"\"\n",
    "Can take a long time to run\n",
    "\n",
    "kValues = [1,2,3,4,4,6,9,15,25,40,60,90]\n",
    "cutoffs = [0.025, 0.05, 0.10, 0.15, 0.20, 0.25, 0.3]\n",
    "numSamples = 10\n",
    "sampleSizes = [20, 50, 100, 150, 250, 500, 1000]\n",
    "\n",
    "lossesList = []\n",
    "for sampleSize in sampleSizes:\n",
    "    lossesList.append(lossOnAllAttributesD([dataNoIDs.sample(sampleSize) for _ in range(numSamples)],\n",
    "                                          [\"Restaurants\"], kValues, cutoffs, sampleSize))\n",
    "\n",
    "\n",
    "import itertools\n",
    "losses = list(itertools.chain.from_iterable(lossesList))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image ref: 04\n",
    "#losses = pd.DataFrame(losses, columns=['Attribute', 'K', 'Cutoff', 'Loss', 'Sample Size'])\n",
    "#sns.relplot(x=\"K\", y=\"Loss\", col=\"Cutoff\", hue=\"Sample Size\", kind=\"line\", col_wrap=3, estimator=None, data=losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View rating distribution per category\n",
    "# Image ref: 03\n",
    "\n",
    "#_=dataNoIDs.hist(figsize=(15,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a function that when applied on the underlying data of the histogram argument \"h\"\n",
    "#     will create an approximately uniform distribution on [0,1].\n",
    "# Assumes histogram is a pair of two arrays where the first one is count in each bin,\n",
    "#     and the second is the bin boundaries.\n",
    "# Also assumes bins are evenly spaced, and that there is at least one bin.\n",
    "def generateUniformizingFunction(h):\n",
    "    # Check array format\n",
    "    assert len(h) == 2, \"Not a valid histogram--should be composed of two arrays\"\n",
    "    assert len(h[0]) > 0 and len(h[1]) > 1, \"Not a valid histogram--should be nonempty\"\n",
    "    assert len(h[0]) + 1 == len(h[1]), \"Not a valid histogram--number of bins does not correspond to number of bin boundaries\"\n",
    "    \n",
    "    hist = [item[:] for item in h]\n",
    "    \n",
    "    numItems = len(h[0])\n",
    "    spaceUnit_0 = h[1][1]-h[1][0]\n",
    "    spaceUnit_1 = 1.0 / float(sum(h[0]))\n",
    "    cumSumNumItems = [sum(h[0][:i]) for i in range(numItems)]\n",
    "    \n",
    "    def getIndex(val):\n",
    "        for i in reversed(range(len(hist[0]))):\n",
    "            if val >= hist[1][i]:\n",
    "                return i\n",
    "        return -1\n",
    "        \n",
    "    def function(val):\n",
    "        i = getIndex(val)\n",
    "        if i == -1:\n",
    "            return val\n",
    "        else:\n",
    "            delta = val - hist[1][i]\n",
    "            disp = delta/spaceUnit_0*spaceUnit_1*hist[0][i]\n",
    "            return cumSumNumItems[i]*spaceUnit_1 + disp\n",
    "            \n",
    "    return function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nkValues = [1,2,3,4,5,8,16,30]\\nlosses = []\\nfor k in kValues:\\n    for attr in [\"Restaurants\", \"View points\", \"Zoo\", \"Churches\"]:\\n        data, vectorSize = convertFrameNormalize(subData, attr)\\n        trainData = data[:int(len(data)*0.8)]\\n        testData = data[int(len(data)*0.8):]\\n        KNN.setData(trainData,vectorSize,k)\\n        losses.append((attr, avgSquareDiffLoss(KNN.predict_averageLabels, testData), k))\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trying sample normalization (so that users with similar \"preference spectra\" will have small distances)\n",
    "# The vectors of ratings for the selected prediction attributes will be normalized, but of course the rating label will\n",
    "#     not be affected\n",
    "\n",
    "subData = ratingsDataMaster.sample(500).drop(\"Unique ID\", axis=1)\n",
    "subData.head().values\n",
    "attributes = subData.columns\n",
    "\n",
    "# Convert a dataframe into vector/label pairs while normalizing vectors.\n",
    "#     Takes \"attr\" as the label and collects the rest of the features as a vector.\n",
    "#     Returns a pair of (data, size), where \"data\" is a list of [vector, label]\n",
    "#        and \"size\" is number of component of vector.\n",
    "# Has option to scale the label down by magnitude of vector\n",
    "# Has option to include norm of vector as component\n",
    "def convertFrameNormalize(data, attr, scaleLabel=False, normAsAttribute=False):\n",
    "    vectors = data.drop(attr, axis=1).values\n",
    "    labels = data.loc[:,attr].values\n",
    "    if scaleLabel:\n",
    "        if normAsAttribute:\n",
    "            return [[np.append(vector/np.linalg.norm(vector), np.array(np.linalg.norm(vector))), label/np.linalg.norm(vector)] for vector, label in zip(vectors, labels)], data.columns.size-1\n",
    "        else:\n",
    "            return [[vector/np.linalg.norm(vector), label/np.linalg.norm(vector)] for vector, label in zip(vectors, labels)], data.columns.size-1\n",
    "    else:\n",
    "        if normAsAttribute:\n",
    "            return [[np.append(vector/np.linalg.norm(vector), np.array(np.linalg.norm(vector))), label] for vector, label in zip(vectors, labels)], data.columns.size-1\n",
    "        else:\n",
    "            return [[vector/np.linalg.norm(vector), label] for vector, label in zip(vectors, labels)], data.columns.size-1\n",
    "\n",
    "# Convert a data frame into vector/label pairs without any normalization\n",
    "#     Takes \"attr\" as the label and collects the rest of the features as a vector.\n",
    "#     Returns a pair of (data, size), where \"data\" is a list of [vector, label]\n",
    "#        and \"size\" is number of component of vector.\n",
    "def convertFrame(data, attr):\n",
    "    currentData = data.drop(attr, axis=1).values\n",
    "    currentLabels = data[attr].values\n",
    "    return [[np.array(example),label] for example, label in zip(currentData,currentLabels)], data.columns.size-1\n",
    "\n",
    "\n",
    "KNN = KNearestNeighbors.KNearestNeighbors()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "k=10\n",
    "lossesByAttribute = []\n",
    "for attr in attributes:\n",
    "    data, vectorSize = convertFrame(subData, attr)\n",
    "    trainData = data[:int(len(data)*0.8)]\n",
    "    testData = data[int(len(data)*0.8):]\n",
    "    KNN.setData(trainData,vectorSize,k)\n",
    "    lossesByAttribute.append((attr, avgSquareDiffLoss(KNN.predict_averageLabels, testData)))\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "kValues = [1,2,3,4,5,8,16,30]\n",
    "losses = []\n",
    "for k in kValues:\n",
    "    for attr in [\"Restaurants\", \"View points\", \"Zoo\", \"Churches\"]:\n",
    "        data, vectorSize = convertFrameNormalize(subData, attr)\n",
    "        trainData = data[:int(len(data)*0.8)]\n",
    "        testData = data[int(len(data)*0.8):]\n",
    "        KNN.setData(trainData,vectorSize,k)\n",
    "        losses.append((attr, avgSquareDiffLoss(KNN.predict_averageLabels, testData), k))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits a list into two disjoint lists\n",
    "# Meant to be used to separate a data sample into a training set and a test set\n",
    "def split_sample(sample, factor=0.8):\n",
    "    return sample[:int(factor * len(sample))], sample[int(factor * len(sample)):]\n",
    "\n",
    "# Define yet another function to calculate loss based on parameters while promising myself I will \n",
    "#     make sure to follow better design practices in the future.\n",
    "\n",
    "# Loss of using KNN with parameter 'k' on 'sample' which should be in format of vector/feature pairs\n",
    "def loss(sample, k, normalized=False):\n",
    "    trainData, testData = split_sample(sample)\n",
    "\n",
    "    vec_size = len(testData[0])\n",
    "    \n",
    "    KNN = KNearestNeighbors.KNearestNeighbors()\n",
    "    KNN.setData(trainData, vec_size, k)\n",
    "    \n",
    "    if normalized:\n",
    "        return avgSquareDiffLoss(KNN.predict_averageLabels_rescale, testData)\n",
    "    else:\n",
    "        return avgSquareDiffLoss(KNN.predict_averageLabels, testData)\n",
    "\n",
    "# Average loss of KKN with parameter 'k' over 'numSamples' samples of data of size 'sampleSize'\n",
    "#     'data' should be a list of vector/label pairs\n",
    "def avgLoss(data, sampleSize, numSamples, k, normalized=False):\n",
    "    lossSum = 0\n",
    "    for _ in range(numSamples):\n",
    "        lossSum += loss(random.sample(data, sampleSize), k, normalized)\n",
    "    return lossSum/float(numSamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.1925367346938775, 0.004643863505408388),\n",
       " (2.3074116326530616, 0.01809838774714556),\n",
       " (0.3994594897959183, 0.017346497426914305),\n",
       " (0.7816003061224491, 0.014744786112695496),\n",
       " (1.2296771428571425, 0.014251333622329507),\n",
       " (0.5796635714285714, 0.00988986750737459),\n",
       " (1.3309897959183674, 0.009031833168569184),\n",
       " (0.5702606122448981, 0.010755718238534622),\n",
       " (0.6007102040816328, 0.012977808564295474),\n",
       " (1.6764723469387754, 0.012685020347811892),\n",
       " (1.3568854196215663, 0.01645676479570583),\n",
       " (1.023602142857143, 0.008537768977747984),\n",
       " (2.015899285714286, 0.01492570451253275),\n",
       " (1.898198469387755, 0.01593505269995689),\n",
       " (1.7535475510204084, 0.011737026117894695),\n",
       " (1.6030885714285712, 0.012193180989837061),\n",
       " (0.3653182653061225, 0.006079194649294027),\n",
       " (1.0689657142857145, 0.000922421748781064),\n",
       " (1.054988775510204, 0.008114575244803154),\n",
       " (0.8904998979591836, 0.003318647629489803),\n",
       " (0.30781969387755104, 0.003918954180473863),\n",
       " (1.3709095918367349, 0.009861602929975889),\n",
       " (1.3000745918367345, 0.008954376265391379),\n",
       " (1.489555918367347, 0.013789148974041581)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "diff = []\n",
    "for attr in dataNoIDs.columns:\n",
    "    D1, _ = convertFrame(dataNoIDs, attr)\n",
    "    D2, _ = convertFrameNormalize(dataNoIDs, attr, normAsAttribute=True, scaleLabel=True)\n",
    "\n",
    "    L1 = avgLoss(D1, 100, 1, 7)\n",
    "    L2 = avgLoss(D2, 100, 1, 7, normalized=True)\n",
    "    diff.append((L1, L2))\n",
    "display(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-881e53baf25a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"Attribute\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Loss\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"K\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"K\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Loss\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Attribute\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"line\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_wrap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'losses' is not defined"
     ]
    }
   ],
   "source": [
    "losses = pd.DataFrame(losses, columns = [\"Attribute\", \"Loss\", \"K\"])\n",
    "sns.relplot(x=\"K\", y=\"Loss\", col=\"Attribute\", kind=\"line\", col_wrap=2, estimator=None, data=losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
